{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Play Generator with RNN .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bvZBCkQ2iRa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59f9b6fe-7add-4aa8-973e-2dadd76c2dc1"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_NyMP456z82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b80d24aa-9ddf-4dfe-9ef6-12ed9d1a7a80"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUQdePPf7IGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "31c30a3a-5006-48e2-b4f1-4de7d0d52749"
      },
      "source": [
        "path_to_file =  tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzbUfcU77px6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Load your own data\n",
        "#from google.colab import files  \n",
        "#path_to_file = list(files.upload().keys())[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3fdLO9P7_Fg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3854b5b-f2c7-4353-ea2f-e5fa0df51e3c"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnbJ8qUO8Ucu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "de47a22f-38ff-40d2-bb1f-1c7257dc65bc"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnk87pA48cap",
        "colab_type": "text"
      },
      "source": [
        "### Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZGDjjXW8Z9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqREA1Wj9AXe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "93ec4887-8621-4a92-c3a3-b4b93be4e950"
      },
      "source": [
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ5TX81H9NN4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc2d87ae-37b7-4fcb-cc95-0aa5825bac87"
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mDoNpwl9f2-",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning Trainig Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7lLHTLM9bpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BJuzBfb9nGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYqmrTaX9uwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccPrEnQ_9z_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "bcad74eb-7f16-4a56-ccba-323becaf9a71"
      },
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1cZLd5n919S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2AcCxMo_FSx",
        "colab_type": "text"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVOca2Nh93xF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYag6CsA_JUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "c647591b-4c8d-40a1-a505-d4c4e66989d8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb6YRh1O_Zlm",
        "colab_type": "text"
      },
      "source": [
        "### Creating Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YObA7od4_Jzl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58477184-e16b-4055-d810-2edac4a6ad04"
      },
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKqpg1or_hIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5eea7368-5690-43f5-8b56-e160267ca47a"
      },
      "source": [
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 1.86759233e-03  1.16635207e-03 -1.26199413e-03 ...  8.30300432e-03\n",
            "   -1.17582572e-03  6.26885146e-03]\n",
            "  [ 3.43566039e-03  2.15381524e-03 -2.20989296e-03 ...  1.48541145e-02\n",
            "   -1.26740034e-03  1.10950591e-02]\n",
            "  [ 1.15922401e-02  2.42568925e-03 -4.69739223e-03 ...  1.20851072e-02\n",
            "   -3.05597857e-03  1.11515103e-02]\n",
            "  ...\n",
            "  [ 1.49235278e-02  1.22489082e-03 -6.39764965e-03 ... -1.18943723e-03\n",
            "   -5.30797895e-03 -1.10750506e-03]\n",
            "  [ 9.53012239e-03  4.62089991e-03  2.36015953e-03 ... -3.89344711e-03\n",
            "   -3.06534325e-03 -1.19954208e-03]\n",
            "  [ 1.54801747e-02  3.19371326e-03 -1.48654915e-03 ... -1.85930182e-03\n",
            "   -4.68004402e-03  1.69091951e-03]]\n",
            "\n",
            " [[-1.35213998e-03  3.78786679e-03  7.18956813e-03 ... -2.11322587e-03\n",
            "   -4.08747117e-04 -8.98380764e-04]\n",
            "  [-4.51066764e-04 -2.08223448e-03  6.33154018e-03 ... -5.69502963e-03\n",
            "    5.08951780e-05 -3.73539561e-03]\n",
            "  [ 8.37184209e-03 -1.65615696e-04  1.05197518e-03 ... -2.51463894e-03\n",
            "   -3.31108691e-03  1.37647381e-04]\n",
            "  ...\n",
            "  [ 7.87501968e-03 -5.71602071e-03 -2.71316618e-03 ...  9.81435180e-04\n",
            "    3.97581258e-04 -2.15789024e-03]\n",
            "  [ 1.01116095e-02 -5.28572779e-03 -8.50022584e-03 ...  2.26866966e-03\n",
            "    4.22035344e-04 -2.60484847e-03]\n",
            "  [ 1.21541731e-02 -7.65664922e-03 -4.35153954e-03 ...  6.95734867e-04\n",
            "    2.78193201e-03 -2.39644898e-03]]\n",
            "\n",
            " [[-1.35213998e-03  3.78786679e-03  7.18956813e-03 ... -2.11322587e-03\n",
            "   -4.08747117e-04 -8.98380764e-04]\n",
            "  [ 4.65789926e-04  3.35403276e-03  3.88091523e-03 ...  7.83793069e-03\n",
            "   -1.50347687e-03  5.37320506e-03]\n",
            "  [ 2.03844253e-03  3.47033935e-03  1.46781211e-03 ...  1.52268447e-02\n",
            "   -1.53798191e-03  1.03196455e-02]\n",
            "  ...\n",
            "  [ 1.49172163e-02 -3.76283936e-03 -7.44650885e-03 ... -4.02789423e-03\n",
            "   -4.77912975e-03 -1.67407375e-03]\n",
            "  [ 1.28927175e-02 -4.43625683e-03 -1.05712768e-02 ... -8.93602148e-03\n",
            "    1.54350465e-03 -5.68644609e-04]\n",
            "  [ 5.44711296e-03 -6.76257070e-03 -6.62507582e-03 ... -6.87562115e-03\n",
            "    5.34703489e-03 -1.10383611e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 3.11123859e-03  7.04754842e-04 -6.12182217e-03 ...  1.79754850e-03\n",
            "   -3.46076442e-04 -4.02265694e-04]\n",
            "  [ 6.05479674e-03 -1.94951473e-03 -2.40232307e-03 ...  7.28713581e-04\n",
            "    1.95922563e-03 -4.45874321e-04]\n",
            "  [ 3.50850588e-03  1.33935211e-03  6.09877054e-03 ... -1.73293974e-03\n",
            "    1.53197255e-03 -5.09181991e-04]\n",
            "  ...\n",
            "  [ 9.64200031e-03  6.43048435e-04 -6.72962610e-03 ... -1.04402257e-02\n",
            "   -1.52499729e-03  1.04068033e-02]\n",
            "  [ 9.73575842e-03  1.20782480e-03 -3.19906184e-03 ... -9.52055305e-03\n",
            "    6.02506660e-03  1.19324494e-02]\n",
            "  [ 1.01649780e-02  3.10472213e-03  1.81919476e-03 ...  1.71183946e-03\n",
            "    6.21691695e-04  1.21331681e-02]]\n",
            "\n",
            " [[-3.58837657e-03  3.44828353e-03 -4.22444288e-03 ... -1.11183326e-03\n",
            "   -5.07792970e-03  7.14836351e-04]\n",
            "  [ 3.08227167e-03  8.40701163e-03 -3.86897940e-04 ...  4.62835375e-03\n",
            "   -2.21243291e-03 -7.86444463e-04]\n",
            "  [ 1.10367294e-02  6.76910114e-03 -3.15133855e-03 ...  3.08261998e-03\n",
            "   -6.64057815e-03  1.70737389e-03]\n",
            "  ...\n",
            "  [ 4.52074129e-03  7.11330213e-05  4.51265369e-05 ...  2.26561911e-03\n",
            "   -4.56696935e-03  3.62283783e-04]\n",
            "  [ 7.33969267e-03  1.74012803e-03  1.78424059e-03 ...  1.19662471e-02\n",
            "   -6.94524543e-03  3.41625372e-03]\n",
            "  [-1.50021748e-04 -8.40856228e-05  1.77034829e-03 ...  1.26279723e-02\n",
            "   -1.84672384e-03  4.66348324e-03]]\n",
            "\n",
            " [[ 8.75734910e-03  6.81207399e-04 -3.11543001e-03 ...  5.73008729e-04\n",
            "   -3.36953485e-03  2.37158593e-03]\n",
            "  [ 3.31136677e-03 -3.39840958e-03 -5.99410851e-04 ...  7.24345562e-04\n",
            "    7.68014812e-04  9.75853298e-04]\n",
            "  [ 6.52647903e-03 -2.59268819e-03 -6.40060473e-03 ...  1.60286925e-03\n",
            "    4.07113694e-05  9.66826919e-05]\n",
            "  ...\n",
            "  [ 8.52600019e-03  1.19357044e-03  4.23694262e-03 ... -9.77884512e-04\n",
            "   -6.53413450e-03 -4.10336442e-03]\n",
            "  [ 1.48379998e-02  1.61759509e-03 -1.03043858e-03 ... -2.09054211e-03\n",
            "   -7.45711941e-03 -1.20327296e-03]\n",
            "  [ 1.44022796e-02  3.33554763e-03  2.11328082e-03 ...  6.51926687e-03\n",
            "   -8.11564736e-03  1.82184810e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWKcuk4q_jlv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "95f26d34-31da-4096-e925-dd24cbb148ff"
      },
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 0.00186759  0.00116635 -0.00126199 ...  0.008303   -0.00117583\n",
            "   0.00626885]\n",
            " [ 0.00343566  0.00215382 -0.00220989 ...  0.01485411 -0.0012674\n",
            "   0.01109506]\n",
            " [ 0.01159224  0.00242569 -0.00469739 ...  0.01208511 -0.00305598\n",
            "   0.01115151]\n",
            " ...\n",
            " [ 0.01492353  0.00122489 -0.00639765 ... -0.00118944 -0.00530798\n",
            "  -0.00110751]\n",
            " [ 0.00953012  0.0046209   0.00236016 ... -0.00389345 -0.00306534\n",
            "  -0.00119954]\n",
            " [ 0.01548017  0.00319371 -0.00148655 ... -0.0018593  -0.00468004\n",
            "   0.00169092]], shape=(100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikXm4iTe_pvP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "614525e9-6c43-4727-fedd-552d300d7854"
      },
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 1.8675923e-03  1.1663521e-03 -1.2619941e-03 -7.7304128e-04\n",
            " -3.6889396e-03 -3.1224226e-03 -6.2512700e-06 -1.0129604e-03\n",
            " -2.3268405e-03 -1.7557553e-03 -4.4967742e-03  4.0311012e-03\n",
            "  3.6162320e-03  3.3651455e-03  2.8898441e-03 -3.0820146e-03\n",
            "  5.2067800e-04  6.9070007e-03 -5.9069847e-03 -2.0456696e-03\n",
            "  2.6308757e-03  1.1827690e-03 -2.1901811e-03  3.5341026e-03\n",
            " -2.4618772e-03  2.9818842e-03  3.7999549e-03  2.9797293e-03\n",
            " -3.5968430e-03 -1.0368756e-03  2.5818821e-03 -1.0411162e-03\n",
            "  9.5435902e-03  1.1606510e-03  1.1052074e-03 -1.6543310e-03\n",
            " -4.8429356e-03  2.8299645e-03  1.8443390e-03 -2.6749824e-03\n",
            "  6.1664334e-04 -2.4219442e-03 -2.4602129e-03  3.8845639e-04\n",
            " -2.1696670e-03  3.1545360e-03  3.8284953e-03 -2.4794876e-03\n",
            "  1.3281568e-04 -1.7505337e-03  6.1780261e-03 -3.8986928e-03\n",
            "  7.1659451e-04  2.6617052e-03 -4.4529377e-03  7.1956581e-03\n",
            "  1.3710214e-03 -8.4228709e-04 -1.9686781e-03  6.8129972e-04\n",
            " -6.4400840e-04  1.7109374e-03  8.3030043e-03 -1.1758257e-03\n",
            "  6.2688515e-03], shape=(65,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX9Ujkbv_tba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6711f652-e127-43e4-a7ca-d436f6e196dd"
      },
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"PuaQh'RWGUPHnyUoplS&Q,F:K CKobHvp&GhmFhNcZ-Hc,xpF:I;nXgx.HX3'I\\n\\nQby IQfEfs.qF RT'csyTkoYdxAm&TTD :Bc\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKsXuDtW_5aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwpqXH8hADzk",
        "colab_type": "text"
      },
      "source": [
        "### Model Compiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bb2pYjI_8dK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOPA3qrOAGiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR8o6QH2AOkA",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbIUzLh5ALcP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee844360-f15f-43d3-aef8-ce30fdbb1e62"
      },
      "source": [
        "history = model.fit(data, epochs=100, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 172 steps\n",
            "Epoch 1/100\n",
            "172/172 [==============================] - 13s 76ms/step - loss: 2.7529\n",
            "Epoch 2/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 2.0721\n",
            "Epoch 3/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.8140\n",
            "Epoch 4/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.6579\n",
            "Epoch 5/100\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 1.5582\n",
            "Epoch 6/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.4902\n",
            "Epoch 7/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.4407\n",
            "Epoch 8/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.4032\n",
            "Epoch 9/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.3721\n",
            "Epoch 10/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.3457\n",
            "Epoch 11/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 1.3220\n",
            "Epoch 12/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 1.2996\n",
            "Epoch 13/100\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 1.2802\n",
            "Epoch 14/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.2598\n",
            "Epoch 15/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.2418\n",
            "Epoch 16/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 1.2225\n",
            "Epoch 17/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 1.2031\n",
            "Epoch 18/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.1843\n",
            "Epoch 19/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.1636\n",
            "Epoch 20/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 1.1429\n",
            "Epoch 21/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 1.1226\n",
            "Epoch 22/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 1.1010\n",
            "Epoch 23/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.0779\n",
            "Epoch 24/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 1.0552\n",
            "Epoch 25/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 1.0309\n",
            "Epoch 26/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 1.0058\n",
            "Epoch 27/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.9804\n",
            "Epoch 28/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.9550\n",
            "Epoch 29/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.9309\n",
            "Epoch 30/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.9057\n",
            "Epoch 31/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.8808\n",
            "Epoch 32/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.8557\n",
            "Epoch 33/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.8306\n",
            "Epoch 34/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.8065\n",
            "Epoch 35/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.7834\n",
            "Epoch 36/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.7624\n",
            "Epoch 37/100\n",
            "172/172 [==============================] - 11s 65ms/step - loss: 0.7404\n",
            "Epoch 38/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.7209\n",
            "Epoch 39/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.7011\n",
            "Epoch 40/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.6840\n",
            "Epoch 41/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.6654\n",
            "Epoch 42/100\n",
            "172/172 [==============================] - 11s 65ms/step - loss: 0.6487\n",
            "Epoch 43/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.6330\n",
            "Epoch 44/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.6192\n",
            "Epoch 45/100\n",
            "172/172 [==============================] - 11s 65ms/step - loss: 0.6044\n",
            "Epoch 46/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.5929\n",
            "Epoch 47/100\n",
            "172/172 [==============================] - 11s 65ms/step - loss: 0.5801\n",
            "Epoch 48/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.5696\n",
            "Epoch 49/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.5575\n",
            "Epoch 50/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.5484\n",
            "Epoch 51/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.5401\n",
            "Epoch 52/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.5304\n",
            "Epoch 53/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.5212\n",
            "Epoch 54/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.5139\n",
            "Epoch 55/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.5058\n",
            "Epoch 56/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.5001\n",
            "Epoch 57/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.4914\n",
            "Epoch 58/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.4875\n",
            "Epoch 59/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.4799\n",
            "Epoch 60/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.4758\n",
            "Epoch 61/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.4685\n",
            "Epoch 62/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.4647\n",
            "Epoch 63/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.4610\n",
            "Epoch 64/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.4544\n",
            "Epoch 65/100\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 0.4521\n",
            "Epoch 66/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.4479\n",
            "Epoch 67/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.4441\n",
            "Epoch 68/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.4409\n",
            "Epoch 69/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.4376\n",
            "Epoch 70/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.4332\n",
            "Epoch 71/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.4301\n",
            "Epoch 72/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.4288\n",
            "Epoch 73/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.4256\n",
            "Epoch 74/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.4219\n",
            "Epoch 75/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.4219\n",
            "Epoch 76/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.4168\n",
            "Epoch 77/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.4164\n",
            "Epoch 78/100\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.4121\n",
            "Epoch 79/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.4112\n",
            "Epoch 80/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.4089\n",
            "Epoch 81/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.4054\n",
            "Epoch 82/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.4058\n",
            "Epoch 83/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.4016\n",
            "Epoch 84/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.4003\n",
            "Epoch 85/100\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 0.3982\n",
            "Epoch 86/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.3965\n",
            "Epoch 87/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.3955\n",
            "Epoch 88/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.3938\n",
            "Epoch 89/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.3943\n",
            "Epoch 90/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.3932\n",
            "Epoch 91/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.3896\n",
            "Epoch 92/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.3883\n",
            "Epoch 93/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.3864\n",
            "Epoch 94/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.3845\n",
            "Epoch 95/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.3842\n",
            "Epoch 96/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.3826\n",
            "Epoch 97/100\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.3820\n",
            "Epoch 98/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.3820\n",
            "Epoch 99/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.3787\n",
            "Epoch 100/100\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.3774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9TTXNmRDNIw",
        "colab_type": "text"
      },
      "source": [
        "### Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCFVmFO-ATyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Ypwnl0DRIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdOxpGc5DT2y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "cfbd0159-274f-4441-fb9c-8ef6b9c7aed2"
      },
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-97614ead033d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./training_checkpoints/ckpt_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1181\u001b[0m           'True when by_name is True.')\n\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m       \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_is_hdf5_filepath\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m   return (filepath.endswith('.h5') or filepath.endswith('.keras') or\n\u001b[0m\u001b[1;32m   1501\u001b[0m           filepath.endswith('.hdf5'))\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python._pywrap_checkpoint_reader.Checkp' object has no attribute 'endswith'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3qWgnhWDcwT",
        "colab_type": "text"
      },
      "source": [
        "### Gnerating Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xi01KgNDWTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oW6gsD7DjwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "ef813117-3315-4f15-f02d-4584f3b22d7b"
      },
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: ok I am legend\n",
            "ok I am legend for her these abundantagable, rain; poor gracious daughter\n",
            "Reed with our loss.\n",
            "\n",
            "Second Citizen:\n",
            "We must thou not garlians, be but sworn my love!\n",
            "butthe baddest.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Ay, but thou wert the king. To shame thee, York\n",
            "Ure sword, cond us, so done!\n",
            "\n",
            "GREMIO:\n",
            "Both holo do grue stile: I'll go along with thee.\n",
            "Now he did not ask, Pompey; in this our noble lord\n",
            "The heavens give body's palm therewith Clarence with her cily news,\n",
            "Swifted to my duty, as my rage,\n",
            "That being laps the wind-short of the death kindred,\n",
            "In whom for Edward's ground, I thought\n",
            "The visitation which we small grow\n",
            "To see my stood standers and my life tood Katharina, the rock, pale\n",
            "Of go a-hood! who's there?\n",
            "\n",
            "BALTHASAR:\n",
            "\n",
            "ISABELLA:\n",
            "I thank you for this ill I swear to way,\n",
            "I shall the queen i' tower;\n",
            "Unless they bear me \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3tXwWsIjPxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}